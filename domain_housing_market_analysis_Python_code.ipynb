{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_text = requests.get('https://www.domain.com.au/liveable-sydney/sydneys-most-liveable-suburbs-2019/sydneys-569-suburbs-ranked-for-liveability-2019-903130/').text\n",
    "soup = BeautifulSoup(website_text)\n",
    "sub_list = []\n",
    "for row in soup.find_all('h3'):\n",
    "    sub_list.append(row.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []\n",
    "for i in sub_list: \n",
    "    if i.split('.')[0].isdigit():\n",
    "        filtered_list.append(i)\n",
    "        \n",
    "lr_list = []\n",
    "f_sub_list = []\n",
    "for i in filtered_list:\n",
    "    lr_list.append(int(i.split('.')[0]))\n",
    "    f_sub_list.append(i.split('.')[1].replace('\\xa0','').strip())\n",
    "\n",
    "\n",
    "sub_dict = {'liveability_ranking':lr_list,'suburb':f_sub_list}\n",
    "\n",
    "df = pd.DataFrame.from_dict(sub_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_list = list(df.suburb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://location-api.domain.com.au/locations/suggestLocations?pageSize=15&prefixText='\n",
    "def get_ref(json_text,a):\n",
    "    if 'Postcode' in json_text[a] and json_text[a]['Postcode'].startswith( '2' ):\n",
    "        return json_text[a]['NameSlug']\n",
    "    else:\n",
    "        a += 1\n",
    "        return get_ref(json_text,a)\n",
    "\n",
    "ref_list =[]\n",
    "        \n",
    "for i in suburb_list:\n",
    "    a = 0\n",
    "    item_url = base_url+i\n",
    "    item_text = requests.get(item_url).text\n",
    "    json_text = json.loads(item_text)\n",
    "    sub_ref = get_ref(json_text,a)\n",
    "    ref_list.append(sub_ref)\n",
    "    \n",
    "\n",
    "df['sub_ref'] = ref_list \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['postcode'] = df['sub_ref'].str[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/final_data/liveability_suburbs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'liveability_ranking'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Suburb Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import pandas as pd\n",
    "\n",
    "con = pg2.connect(host='this_postgres',\n",
    "                  user='postgres',\n",
    "                  password='password',\n",
    "                  database='postgres')\n",
    "con.autocommit = True\n",
    "cur = con.cursor()\n",
    "\n",
    "def select(sql):\n",
    "    return pd.read_sql(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select sub_ref from liveability_ranking'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref = select(sql)\n",
    "ref_list = list(df_ref.sub_ref)\n",
    "ref_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suburb Profile Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w',encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def append_file(file_name, content):\n",
    "    with open(file_name,'a',encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def get_or_download(url,file_name):\n",
    "\n",
    "    if os.path.isfile(file_name):\n",
    "        print (f'{url} already exists as {file_name}')\n",
    "        return read_file(file_name)\n",
    "    else:\n",
    "        content = requests.get(url).text\n",
    "        write_file(file_name,content)\n",
    "        print (f'{url} downloaded to {file_name}')\n",
    "        return read_file(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.domain.com.au/suburb-profile/'\n",
    "for item in ref_list:\n",
    "    get_or_download(base_url+item,f'../suburb_profile/{item}.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suburb Profile Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "def get_profile_feature(sect,feature_name):\n",
    "    for feature in sect.select('div.css-48zwbo') :\n",
    "        if feature_name in feature.getText():\n",
    "            return feature.select('div')[0].getText()\n",
    "        \n",
    "def get_ratios(sect):\n",
    "    container = sect.select('div.css-16g4kqh')[0]\n",
    "    ratio_dict = {}\n",
    "    for item in container.select('div.css-14hea9r'):\n",
    "        left_text = item.select('span[data-testid=\"left-text\"]')[0].getText()\n",
    "        left_value = item.select('span[data-testid=\"left-value\"]')[0].getText()\n",
    "        right_text = item.select('span[data-testid=\"right-text\"]')[0].getText()\n",
    "        right_value = item.select('span[data-testid=\"right-value\"]')[0].getText()\n",
    "        ratio_dict[left_text]=int(re.findall(r'\\d+',left_value)[0])\n",
    "        ratio_dict[right_text]=int(re.findall(r'\\d+',right_value)[0])\n",
    "    return ratio_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = []\n",
    "tabs_ratio = []\n",
    "for sub_file in glob.glob('../suburb_profile/*.html'):\n",
    "    soup = BeautifulSoup(read_file(sub_file))\n",
    "    table = soup.find(\"table\", {\"aria-describedby\":\"market-data-context\"})\n",
    "    if table:\n",
    "        suburb = sub_file.split('/')[2].split('nsw')[0][:-1].replace('-',' ')\n",
    "        sub_ref = sub_file.split('/')[2].split('.')[0]        \n",
    "        sect = soup.select('section#demographics')\n",
    "        if sect:\n",
    "            sect = sect[0]\n",
    "            population_t = get_profile_feature(sect,'Population')\n",
    "            population = int(population_t.replace(',',''))\n",
    "            average_age = get_profile_feature(sect,'Average age')\n",
    "            age_list = average_age.split('to')\n",
    "            if len(age_list) == 1:\n",
    "                min_age = int(re.findall(r'\\d+',age_list[0])[0])\n",
    "                max_age = None\n",
    "            elif len(age_list) == 2:\n",
    "                min_age = int(re.findall(r'\\d+',age_list[0])[0])\n",
    "                max_age = int(re.findall(r'\\d+',age_list[1])[0])\n",
    "            ratio_dictionary = get_ratios(sect)\n",
    "            ratio_dictionary['population'] = population\n",
    "            ratio_dictionary['min_age'] = min_age\n",
    "            ratio_dictionary['max_age'] = max_age\n",
    "            ratio_dictionary['suburb'] = suburb\n",
    "            ratio_dictionary['sub_ref'] = sub_ref\n",
    "            tabs_ratio.append(ratio_dictionary)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        trs = table.find_all('tr')\n",
    "        for tr in trs:\n",
    "            if tr.find_all('td'):\n",
    "                beds_num = int(tr.find_all('td')[0].getText())\n",
    "                m_price_text = tr.find_all('td')[2].getText()\n",
    "                price_num_s = re.findall(r'[\\d|\\.]+',m_price_text)[0] if re.findall(r'[\\d|\\.]+',m_price_text) else None\n",
    "                if 'm' in m_price_text:\n",
    "                    price = float(price_num_s)*1000000\n",
    "                elif 'k' in m_price_text:\n",
    "                    price = float(price_num_s)*1000\n",
    "                else:\n",
    "                    price = float(price_num_s) if price_num_s else None\n",
    "                days_text = tr.find_all('td')[3].getText()\n",
    "                days = int(re.findall(r'\\d+',days_text)[0]) if re.findall(r'\\d+',days_text) else None\n",
    "                c_rate_text = tr.find_all('td')[4].getText()\n",
    "                c_rate = int(re.findall(r'\\d+',c_rate_text)[0]) if re.findall(r'\\d+',c_rate_text) else None\n",
    "                sold_last_12 = int(tr.find_all('td')[5].getText())\n",
    "                \n",
    "\n",
    "                \n",
    "                tabs.append({\n",
    "                    'bedrooms':beds_num,\n",
    "                    'type':tr.find_all('td')[1].getText(),\n",
    "                    'median_price':price,\n",
    "                    'avg_days_on_market':days,\n",
    "                    'clearance_rate':c_rate,\n",
    "                    'sold_last_12_months':sold_last_12,\n",
    "                    'Suburb':suburb,\n",
    "                    'sub_ref':sub_ref\n",
    "                })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(tabs)\n",
    "\n",
    "df_ratios = pd.DataFrame(tabs_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/final_data/suburbs profile.csv')\n",
    "\n",
    "df_ratios.to_csv('../data/final_data/suburbs ratios.csv')\n",
    "\n",
    "from sqlalchemy import create_engine,types\n",
    "\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'suburb_profile'\n",
    "df.to_sql(name, engine, if_exists='replace',\n",
    "         dtype={\n",
    "             'bedrooms':types.INTEGER(),\n",
    "             'median_price':types.INTEGER(),\n",
    "             'avg_days_on_market':types.INTEGER(),\n",
    "             'clearance_rate':types.INTEGER(),\n",
    "             'sold_last_12_months':types.INTEGER(),\n",
    "             'type':types.VARCHAR(),\n",
    "             'Suburb':types.VARCHAR(),\n",
    "             'sub_ref':types.VARCHAR()\n",
    "         })\n",
    "\n",
    "name = 'suburb_ratios'\n",
    "df_ratios.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all suburbs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import pandas as pd\n",
    "con = pg2.connect(host='this_postgres',\n",
    "                  user='postgres',\n",
    "                  password='password',\n",
    "                  database='postgres')\n",
    "con.autocommit = True\n",
    "cur = con.cursor()\n",
    "\n",
    "def select(sql):\n",
    "    return pd.read_sql(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql1 = '''select sum(sold_last_12_months) as Sold,\"Suburb\",\"sub_ref\",\"type\"\n",
    "# from suburb_profile\n",
    "# where type = 'House'\n",
    "# group by \"Suburb\",\"sub_ref\",type\n",
    "# order by Sold desc\n",
    "# limit 50'''\n",
    "\n",
    "\n",
    "sql = '''select sum(sold_last_12_months) as Sold,\"Suburb\",\"sub_ref\"\n",
    "from suburb_profile\n",
    "group by \"Suburb\",\"sub_ref\"\n",
    "order by Sold desc'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = select(sql1)\n",
    "df = select(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine,types\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'chosen_suburbs_all'\n",
    "df.to_sql(name, engine, if_exists='replace')\n",
    "# name1 = 'chosen_suburbs_50' \n",
    "# df1.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('../data/final_data/50 chosen suburbs.csv')\n",
    "df.to_csv('../data/final_data/all chosen suburbs.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Chosen Suburb Sold/On Sale data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Suburb Sold Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page_url(page):\n",
    "    next_button_selector = page.select('a[data-testid=\"paginator-navigation-button\"]')\n",
    "    for item in next_button_selector:\n",
    "        button_text = item.getText()\n",
    "        if button_text == 'next page':\n",
    "            return item.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_download(url,file_name):\n",
    "\n",
    "    if os.path.isfile(file_name):\n",
    "        print (f'{url} already exists as {file_name}')\n",
    "        return read_file(file_name)\n",
    "    else:\n",
    "        content = requests.get(url).text\n",
    "        write_file(file_name,content)\n",
    "        print (f'{url} downloaded to {file_name}')\n",
    "        return read_file(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_urls(base_url,page_url,get_next_page_url):\n",
    "\n",
    "    html = requests.get(base_url + page_url).text\n",
    "    page = BeautifulSoup(html)\n",
    "    current_urls = set([base_url + page_url])\n",
    "    next_page_url = get_next_page_url(page)\n",
    "        \n",
    "    if next_page_url:\n",
    "        return current_urls | get_urls(base_url,next_page_url,get_next_page_url)\n",
    "    else:\n",
    "        return current_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.domain.com.au'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in ref_list:\n",
    "    print(ref)\n",
    "    start_page = f'/sold-listings/{ref}/?excludepricewithheld=1&ssubs=0&page=1'\n",
    "    for urls in  get_urls(base_url,start_page,get_next_page_url):\n",
    "        page_number = urls.split('=')[-1]\n",
    "        get_or_download(urls,f'../domain_sold/{ref}-{page_number}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Suburb Sold Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "def ifisempty(v):\n",
    "    if v != []:\n",
    "        return v[0]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_text_to_num(m):\n",
    "    if m.lower() in ['jan' , 'january']:\n",
    "        m = '01'\n",
    "    elif  m.lower() in ['feb' , 'february']:\n",
    "        m = '02'\n",
    "    elif  m.lower() in ['mar' , 'march']:\n",
    "        m = '03'\n",
    "    elif  m.lower() in ['apr' , 'april']:\n",
    "        m = '04'\n",
    "    elif  m.lower() in ['may']:\n",
    "        m = '05'\n",
    "    elif  m.lower() in ['jun' , 'june']:\n",
    "        m = '06'\n",
    "    elif  m.lower() in ['jul' , 'july']:\n",
    "        m = '07'\n",
    "    elif  m.lower() in ['aug' , 'august']:\n",
    "        m = '08'\n",
    "    elif  m.lower() in ['sep' , 'september']:\n",
    "        m = '09'\n",
    "    elif  m.lower() in ['oct' , 'october']:\n",
    "        m = '10'\n",
    "    elif  m.lower() in ['nov' , 'november']:\n",
    "        m = '11'\n",
    "    elif  m.lower() in ['dec' , 'december']:\n",
    "        m = '12'\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = []\n",
    "for file_name in glob.glob('../domain_sold/*.html'):\n",
    "    print(f'running:{file_name}')\n",
    "    page = BeautifulSoup(read_file(file_name))\n",
    "    house_cards = page.select('div[data-testid^=\"listing-card-wrapper\"]')\n",
    "    for card in house_cards:\n",
    "        price_t = card.select('p[data-testid=\"listing-card-price\"]')[0].getText() or ''\n",
    "        price = int(re.findall(r'([\\d,]+)',price_t)[0].replace(',',''))\n",
    "        address = card.select('a.address')[0].find(itemprop=\"name\").get(\"content\")\n",
    "        suburb = card.select('span[data-testid=\"address-line2\"]')[0].getText() or ''\n",
    "        sub_ref = suburb.replace(' ','-').lower()\n",
    "        sold_date_tag = card.select('div[data-testid=\"listing-card-tag\"]')[0].getText() or ''\n",
    "        if 'Sold at auction' in sold_date_tag:\n",
    "            sold_type = 'Auction'\n",
    "        else:\n",
    "            sold_type = 'Private Treaty'\n",
    "        sold_date = sold_date_tag.split()[-1]+'-'+month_text_to_num(sold_date_tag.split()[-2])+'-'+sold_date_tag.split()[-3]\n",
    "        try:\n",
    "            sold_date = datetime.strptime(sold_date, \"%Y-%m-%d\").date()\n",
    "        except:\n",
    "            sold_date = None\n",
    "\n",
    "        \n",
    "        features = card.select('div[data-testid=\"listing-card-features-wrapper\"]')[0].getText()\n",
    "\n",
    "        beds = int(re.findall(r'(\\d) Bed',features)[0]) if re.findall(r'(\\d) Bed',features) else None\n",
    "        baths = int(re.findall(r'(\\d) Bath',features)[0]) if re.findall(r'(\\d) Bath',features) else None\n",
    "        parks = int(re.findall(r'(\\d) Park',features)[0]) if re.findall(r'(\\d) Park',features) else None\n",
    "        lands = int(re.findall(r'(\\d+)m²',features)[0]) if re.findall(r'(\\d+)m²',features) else None\n",
    "\n",
    "        property_type = card.select('span[class=\"css-693528\"]')[0].getText() or ''\n",
    "\n",
    "        links = card.select('link[itemprop=\"url\"]')\n",
    "        for urls in links:\n",
    "            url = urls.attrs['href']\n",
    "            listing_id = int(re.findall(r'^(\\d+)',url.split('-')[-1])[0])\n",
    "\n",
    "            \n",
    "        tabs.append({\n",
    "            'sold_price':price,\n",
    "            'sold_price_desc':price_t,\n",
    "            'address':address,\n",
    "            'suburb':suburb,\n",
    "            'sub_ref':sub_ref,\n",
    "            'sold_type':sold_type,\n",
    "            'sold_date':sold_date,\n",
    "            'bedrooms':beds,\n",
    "            'bathrooms':baths,\n",
    "            'parkings':parks,\n",
    "            'landsize_m²':lands,\n",
    "            'property_type':property_type,\n",
    "            'url':url,\n",
    "            'listing_id':listing_id\n",
    "            \n",
    "        })\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['listing_id','address','suburb','sub_ref','sold_date','sold_type','property_type','sold_price','sold_price_desc',  'bedrooms', 'bathrooms', 'parkings', 'landsize_m²', 'url']\n",
    "df = pd.DataFrame(tabs)[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/final_data/sold_history_all_subs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine,types\n",
    "\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'sold_history_all_subs'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Suburb On Sale Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page_url(page):\n",
    "    next_button_selector = page.select('a[data-testid=\"paginator-navigation-button\"]')\n",
    "    for item in next_button_selector:\n",
    "        button_text = item.getText()\n",
    "        if button_text == 'next page':\n",
    "            return item.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_download(url,file_name):\n",
    "\n",
    "    if os.path.isfile(file_name):\n",
    "        print (f'{url} already exists as {file_name}')\n",
    "        return read_file(file_name)\n",
    "    else:\n",
    "        content = requests.get(url).text\n",
    "        write_file(file_name,content)\n",
    "        print (f'{url} downloaded to {file_name}')\n",
    "        return read_file(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_urls(base_url,page_url,get_next_page_url):\n",
    "\n",
    "    html = requests.get(base_url + page_url).text\n",
    "    page = BeautifulSoup(html)\n",
    "    current_urls = set([base_url + page_url])\n",
    "    next_page_url = get_next_page_url(page)\n",
    "        \n",
    "    if next_page_url:\n",
    "        return current_urls | get_urls(base_url,next_page_url,get_next_page_url)\n",
    "    else:\n",
    "        return current_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import pandas as pd\n",
    "con = pg2.connect(host='this_postgres',\n",
    "                  user='postgres',\n",
    "                  password='password',\n",
    "                  database='postgres')\n",
    "con.autocommit = True\n",
    "cur = con.cursor()\n",
    "\n",
    "def select(sql):\n",
    "    return pd.read_sql(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select sub_ref from chosen_suburbs_all'''\n",
    "df = select(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list = list(df.sub_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.domain.com.au'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in ref_list:\n",
    "    start_page = f'/sale/{ref}/?excludeunderoffer=1&ssubs=0&page=1'\n",
    "    print(ref)\n",
    "    for urls in  get_urls(base_url,start_page,get_next_page_url):\n",
    "        page_number = urls.split('=')[-1]\n",
    "        get_or_download(urls,f'../domain_on_sale/{ref}-{page_number}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Suburb On Sale Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "    \n",
    "    \n",
    "def get_num(price):\n",
    "    price_num = int(''.join(re.findall(r'\\d+', price)))\n",
    "    return  price_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tabs = []\n",
    "\n",
    "for file_name in glob.glob('../domain_on_sale/*.html'):\n",
    "    print(file_name)\n",
    "    page = BeautifulSoup(read_file(file_name))\n",
    "    house_cards = page.select('div[data-testid^=\"listing-card-wrapper\"]')\n",
    "    for card in house_cards:\n",
    "        price_t = card.select('p[data-testid=\"listing-card-price\"]')[0].getText() or ''\n",
    "        fixed_price = re.findall(r'^\\$[\\d,]+$',price_t)\n",
    "        if fixed_price: \n",
    "            fixed_price = get_num(fixed_price[0])\n",
    "        else:\n",
    "            fixed_price = None\n",
    "        if price_t:\n",
    "            if fixed_price is None and len(re.findall(r'[\\d,]{5,}',price_t)) == 1:\n",
    "                min_price = get_num(re.findall(r'[\\d,]{5,}',price_t)[0])\n",
    "                max_price = None\n",
    "            elif fixed_price is None and len(re.findall(r'[\\d,]{5,}',price_t)) == 2:\n",
    "                min_price = get_num(re.findall(r'[\\d,]{5,}',price_t)[0])\n",
    "                max_price = get_num(re.findall(r'[\\d,]{5,}',price_t)[1])\n",
    "            else:\n",
    "                min_price = None\n",
    "                max_price = None\n",
    "        else:\n",
    "            min_price = None\n",
    "            max_price = None\n",
    "\n",
    "        address = card.select('h2[data-testid=\"address-wrapper\"]')[0].getText() or ''\n",
    "        suburb = card.select('span[data-testid=\"address-line2\"]')[0].getText() or ''\n",
    "        sub_ref = suburb.replace(' ','-').lower()\n",
    "        features = card.select('div[data-testid=\"listing-card-features-wrapper\"]')[0].getText()\n",
    "\n",
    "        beds = int(re.findall(r'(\\d) Bed',features)[0]) if re.findall(r'(\\d) Bed',features) else None\n",
    "        baths = int(re.findall(r'(\\d) Bath',features)[0]) if re.findall(r'(\\d) Bath',features) else None\n",
    "        parks = int(re.findall(r'(\\d) Park',features)[0]) if re.findall(r'(\\d) Park',features) else None\n",
    "        lands = int(re.findall(r'(\\d+)m²',features)[0]) if re.findall(r'(\\d+)m²',features) else None\n",
    "\n",
    "\n",
    "        property_type = features.split()[-1]\n",
    "        property_type = property_type.replace('Parking','')\n",
    "\n",
    "        links = card.select('link[itemprop=\"url\"]')\n",
    "        for urls in links:\n",
    "            url = urls.attrs['href']\n",
    "            try:\n",
    "                listing_id = int(re.findall(r'^(\\d+)',url.split('-')[-1])[0])\n",
    "            except:\n",
    "                listing_id = None\n",
    "\n",
    "        \n",
    "                    \n",
    "        tabs.append({\n",
    "            'fixed_price':fixed_price,\n",
    "            'min_price':min_price,\n",
    "            'max_price':max_price,\n",
    "            'address':address,\n",
    "            'suburb':suburb,\n",
    "            'sub_ref':sub_ref,\n",
    "            'bedrooms':beds,\n",
    "            'bathrooms':baths,\n",
    "            'parkings':parks,\n",
    "            'landsize_m²':lands,\n",
    "            'property_type':property_type,\n",
    "            'url':url,\n",
    "            'listing_id':listing_id\n",
    "            \n",
    "        })\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['listing_id','address', 'suburb', 'sub_ref','property_type', 'fixed_price','min_price','max_price', 'bedrooms', 'bathrooms', 'parkings', 'landsize_m²', 'url']\n",
    "df = pd.DataFrame(tabs)[cols]\n",
    "\n",
    "df = df[df.listing_id.notnull()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/final_data/on_sale_properties_all_subs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine,types\n",
    "\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'on_sale_properties_all_subs'\n",
    "df.to_sql(name, engine, if_exists='replace',dtype={\n",
    "             'fixed_price':types.INTEGER()\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haversine (lat & long) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/michalsn/australian-suburbs/master/data/suburbs.json').text\n",
    "\n",
    "data = json.loads(response)\n",
    "data = data.get('data')\n",
    "\n",
    "df = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_df = df[(2000<=df.postcode) & (df.postcode < 2800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Distance to CBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syd_subs_data = sydney_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syd_cbd_coordinates = (151.2073,-33.8708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in syd_subs_data:\n",
    "    sub_ref = sub['suburb'].lower().replace(' ','-') +'-'+sub['state'].lower()+'-'+ str(sub['postcode'])\n",
    "    distance_to_cbd = haversine((sub['lng'],sub['lat']),syd_cbd_coordinates)\n",
    "    sub['distance_to_cbd'] = round(distance_to_cbd,2)\n",
    "    sub['sub_ref'] = sub_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_df = pd.DataFrame.from_records(syd_subs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_df.to_csv('../data/final_data/sydney_suburbs_lat_lng_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine,types\n",
    "\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'syd_subs_lat_lng'\n",
    "sydney_df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrounding Suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import pandas as pd\n",
    "\n",
    "con = pg2.connect(host='this_postgres',\n",
    "                  user='postgres',\n",
    "                  password='password',\n",
    "                  database='postgres')\n",
    "con.autocommit = True\n",
    "cur = con.cursor()\n",
    "\n",
    "def select(sql):\n",
    "    return pd.read_sql(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select * from syd_subs_lat_lng'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = select(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = set(df.suburb)\n",
    "subs = list(subs)\n",
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syd_subs_data = df.to_dict('records')\n",
    "final_data = []\n",
    "for sub in subs:\n",
    "    subs_recommended = list(filter(lambda s: sub in s['suburb'],syd_subs_data))[0]\n",
    "    coordinates = (float(subs_recommended['lng']),float(subs_recommended['lat']))\n",
    "    get_10km_subs = lambda s: coordinates[0] -0.09 < float(s['lng']) < coordinates[0] + 0.09 and coordinates[1] -0.103 < float(s['lat']) < coordinates[1] + 0.103\n",
    "    sorrunding_subs = list(filter(get_10km_subs,syd_subs_data))\n",
    "    hav = lambda s: haversine(coordinates,(float(s['lng']),float(s['lat'])))\n",
    "    surrounding_data = [(sub,subs_recommended['sub_ref'],s['suburb'],s['sub_ref'],round(hav(s),2)) for s in sorrunding_subs]\n",
    "# #     sorted_data = sorted(surrounding_data, key=lambda tup: tup[2])[1:11]\n",
    "    \n",
    "\n",
    "    [final_data.append(i) for i in surrounding_data]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(final_data,columns=['selected_suburb','selected_suburb_ref', 'surrounding_suburbs','surrounding_suburb_ref', 'distance'])\n",
    "\n",
    "df.to_csv('../data/final_data/surrounding_suburbs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'surrounding_suburbs'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
